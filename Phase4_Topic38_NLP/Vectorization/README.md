# Vectorization

The last steps in preparing our texts for modeling are stemming/lemmatizing and vectorizing.

## Learning Goals

- Lexicon normalization: lemmatization and stemming
- Run feature engineering algorithms for NLP
    - Bag-of-Words
    - Count Vectorizer
    - Term frequency-Inverse Document Frequency (tf-idf)

## Lesson Materials

- [Jupyter Notebook: NLP Vectorization](nlp_vectorization.ipynb)

## Lesson Plan

Provide a breakdown of the lecture activities using the structure below. 

### Introduction (5 Mins)

Almost ready for modeling!

### Stemming (10 Mins)

Porter and Snowball Stemmers are similar but distinct forms of grabbing the root of a word.

### Lemmatizing (20 Mins)

The key difference here is to try to take advantage of grammatical rules *as tied to particular parts of speech*.

### Bag of Words Model (5 Mins)

### Vectorizers (15 Mins)

TF-IDF is probably the go-to, all else equal, but it should be clear that each of the three presented has its uses.

### Conclusion (5 Mins)

Finally ready to model!

## Additional Resources

### Lecture Supplements

Martin Porter's retro [homepage](https://tartarus.org/martin/PorterStemmer/)
