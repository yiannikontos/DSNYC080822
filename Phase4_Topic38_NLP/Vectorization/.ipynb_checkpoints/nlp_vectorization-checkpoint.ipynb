{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Natural Language Processing: Vectorization\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Feb 2022\n",
    "<p>Phase 4: Topic 38</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Notice that these vectorizers are from `sklearn` and not `nltk`!\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,\\\n",
    "HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noting that the resignation of James Mattis as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desperate to unwind after months of nonstop wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nearly halfway through his presidential term, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attempting to make amends for gross abuses of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decrying the Senate’s resolution blaming the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  target\n",
       "0  Noting that the resignation of James Mattis as...       1\n",
       "1  Desperate to unwind after months of nonstop wo...       1\n",
       "2  Nearly halfway through his presidential term, ...       1\n",
       "3  Attempting to make amends for gross abuses of ...       1\n",
       "4  Decrying the Senate’s resolution blaming the c...       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire_df = pd.read_csv(\n",
    "    'data/satire_nosatire.csv')\n",
    "satire_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note resignation james mattis secretary defens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desperate unwind month nonstop work investigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nearly halfway presidential term donald trump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attempt make amends gross abuse power time int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>decry senate resolution blame crown prince bru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>britain opposition leader jeremy corbyn push a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>turkey take fight islamic state militant syria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>malaysia seek reparation goldman sachs group i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>israeli court sentence palestinian year impris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>least people die due landslide flood trigger t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  body\n",
       "0    note resignation james mattis secretary defens...\n",
       "1    desperate unwind month nonstop work investigat...\n",
       "2    nearly halfway presidential term donald trump ...\n",
       "3    attempt make amends gross abuse power time int...\n",
       "4    decry senate resolution blame crown prince bru...\n",
       "..                                                 ...\n",
       "995  britain opposition leader jeremy corbyn push a...\n",
       "996  turkey take fight islamic state militant syria...\n",
       "997  malaysia seek reparation goldman sachs group i...\n",
       "998  israeli court sentence palestinian year impris...\n",
       "999  least people die due landslide flood trigger t...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(\n",
    "    'data/satire_norm.csv').drop(\n",
    "    columns = ['Unnamed: 0'])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "#### Feature Extraction for NLP\n",
    "\n",
    "- learn vector representation of tokenized data\n",
    "- representing text in form for ML model:\n",
    "    - encoding semantic information in numeric form\n",
    "- A simple (yet surprisingly effective) method for many tasks: **Bag-of-words (BoW)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "\"Bag\" of words: **information about the order of words in the document discarded**. \n",
    "\n",
    "- Intuition behind BoW: documents similar if they have similar token frequency distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Represented as **document-term matrix**:\n",
    "- columns are tokens\n",
    "- rows are documents\n",
    "- values are token counts for given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "- Document 1: \"I love dogs.\"\n",
    "- Document 2: \"I love cats.\"\n",
    "- Document 3: \"I love all animals.\"\n",
    "- Document 4: \"I hate dogs.\"\n",
    "\n",
    "This corpus represented in BoW as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "$\\downarrow$Doc\\|Word$\\rightarrow$|I|love|dogs|cats|all|animals|hate\n",
    "-|-|-|-|-|-|-|-\n",
    "Document_1|1|1|1|0|0|0|0\n",
    "Document_2|1|1|0|1|0|0|0\n",
    "Document_3|1|1|0|0|1|1|0\n",
    "Document_4|1|0|1|0|0|0|1\n",
    "<center> Document-term frequency matric</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In order to get these tokens from our documents, we're going to use tools called \"vectorizers\".\n",
    "\n",
    "The most straightforward vectorizer in `sklearn.feature_extraction.text` is the `CountVectorizer`, which will simply count the number of each word type in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing it in python\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform([\" \".join(sample_doc_lemmed)])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not very exciting for one document. The idea is to make a document term matrix for all of the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = vec.fit_transform(corpus.body[1:3])\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw,\n",
    "                      ngram_range=[1, 2])\n",
    "X = vec.fit_transform(corpus.body[0:2])\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our document term matrix gets bigger and bigger, with more and more zeros, becoming sparser and sparser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw,\n",
    "                      ngram_range=[1, 2])\n",
    "# Now fit to the entire corpus\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set upper and lower limits to the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                      stop_words=sw, ngram_range=[1, 2],\n",
    "                      min_df=2, max_df=25)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TfidfVectorizer`\n",
    "\n",
    "There are many schemas for determining the values of each entry in a document term matrix, and one of the most common uses the TF-IDF algorithm -- \"Term Frequency-Inverse Document Frequency\". Essentially, tf-idf *normalizes* the raw count of the document term matrix. And it represents how important a word is in the given document. \n",
    "\n",
    "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "- TF (Term Frequency)\n",
    "Term frequency is the frequency of the word in the document divided by the total words in the document.\n",
    "\n",
    "- IDF (inverse document frequency)\n",
    "Inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is generally calculated as the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "$$idf(w) = log (\\frac{number\\ of\\ documents}{num\\ of\\ documents\\ containing\\ w})$$\n",
    "\n",
    "tf-idf is the product of term frequency and inverse document frequency, or tf * idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = tf_vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=tf_vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[313].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the tfidf to the count vectorizer output for one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf lessoned the importance of some of the more common words, including a word, \"also\", which might have made it into the stopword list.\n",
    "\n",
    "It also assigns \"nerds\" more weight than power.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\"Nerds\" only shows up in document 313: {len(df_cv[df.nerds!=0])} document.')\n",
    "print(f'\"Power\" shows up in {len(df_cv[df.power!=0])} documents!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words are stored in a `.vocabulary_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `HashingVectorizer`\n",
    "\n",
    "There is also a hashing vectorizer, which will encrypt all the words of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvec = HashingVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                         stop_words=sw)\n",
    "X = hvec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rules of thumb about these vectorizers:\n",
    "\n",
    "**Tf-Idf**: Probably the most commonly used. Useful when the goal is to distinguish the **content** of documents from others in the corpus.\n",
    "\n",
    "**Count**: Useful when the words themselves matter. If the goal is instead about identifying authors by their words, then the fact that some word appears in many documents of the corpus may be important.\n",
    "\n",
    "**Hashing**: The advantage here is speed and low memory usage. The disadvantage is that you lose the identities of the words being tokenized. Useful for very large datasets where the ultimate model may be a bit of a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "For a final exercise, work through the following:\n",
    "\n",
    "Create a document term matrix of the 1000-document corpus. The vocabulary should have no stopwords, numbers, or punctuation, and it should be lemmatized. Use a `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    <code># Tokenizing\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in corpus['body']]\n",
    "lower_docs = [[token.lower() for token in doc] for doc in tokenized_docs]\n",
    "sw_docs = [[token for token in doc if token not in sw] for doc in lower_docs]\n",
    "# Initial tagging\n",
    "docs_tagged = [pos_tag(doc) for doc in sw_docs]\n",
    "# Tag with Wordnet tags\n",
    "wordnet_docs_tagged = [[(token[0], get_wordnet_pos(token[1]))\n",
    "             for token in doc] for doc in docs_tagged]\n",
    "# Lemmatize\n",
    "docs_lemmed = [[lemmatizer.lemmatize(token[0], token[1]) for token in doc]\\\n",
    "               for doc in wordnet_docs_tagged]\n",
    "# Use the tf-idf vectorizer to create the matrix\n",
    "X = tf_vec.fit_transform([' '.join(doc) for doc in docs_lemmed])\n",
    "df = pd.DataFrame(X.toarray(), columns=tf_vec.get_feature_names())</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
